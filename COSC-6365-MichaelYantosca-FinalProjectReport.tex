\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{alltt}
\usepackage[utf8]{luainputenc}
\usepackage[bibencoding=utf8,backend=biber]{biblatex}
\addbibresource{COSC-6365-MichaelYantosca-FinalProjectReport.bib}
\pagestyle{fancy}
\fancyhf{}
\rhead{COSC-6365 Final Project Proposal}
\lhead{Michael Yantosca}
\rfoot{\thepage}

\begin{document}
\begin{section}{Abstract}
  To explore the relative merits between single-node computer power in depth versus multi-node compute power in breadth,
  I propose an experiment comparing the performance and efficiency of Cannon's algorithm\autocites{Lecture17Slides,GuptaSadayappan} for the matrix multiplication
  problem $C = A \times B$ between a single-node OpenMP implementation utilizing accelerators and a multi-node MPI
  implementation spread across a cluster.
  \begin{subsection}{Data}
    For the sake of time, I will only consider the $C$-stationary case for a limited number of matrix sizes
    (256x256, 1024x1024, 4096x4096, 16384x16384, 256x1024, 256x4096, 256x16384, 16384x256, 16384x256, 4096x256, 1024x256).
    The matrices will be filled with random numbers for the performance trials. Validation trials will be made
    through specially devised matrices $A$ and $B$ such that each cell of the product matrix $C$ will have a unique value.
  \end{subsection}
  \begin{subsection}{Resources}
    The OpenMP trials will be executed on the GPU nodes on BRIDGES\footnote{If the GPU nodes on BRIDGES do not support
    OpenMP offloading to the P100 and K80 GPUs, the acceleration will be done via OpenACC, or, as a last resort, CUDA.}
    and/or the KNL nodes on STAMPEDE2, whereas the MPI trials will be executed on the appropriate non-accelerated nodes
    on BRIDGES.
  \end{subsection}
  \begin{subsection}{Efficiency}
    The efficiency of the MPI trials will be gauged against a strong-scaling roofline model based on the physical
    characteristics of a single non-GPU node on BRIDGES. The processor in this case is the 28-core, 2.30 GHz Intel E5-2695,
    which is theoretically capable of 2.30 GHz * 28 cores * 4 SIMD instructions/cycle (AVX256) = 257.6 GFLOPs/s/node.
    \begin{paragraph}{}
      The efficiency of the OpenMP trials will be gauged against a roofline model based on the physical characteristics of the accelerator.
      The NVIDIA P100 is theoretically capable of 9.3 SP TFLOPs/s/card\autocite{P100Datasheet}, whereas the NVIDIA K80 is theoretically capable of
      8.74 TFLOPs/s/card\autocite{AnandtechK80}. In the event that KNL nodes on STAMPEDE2 are utilized, the theoretical roofline will
      be considered as 1.4 GHz * 68 cores * 8 SIMD instructions/cycle (AVX512) = 761.6 GFLOPs/s/node\autocite{Stampede2UserGuide}\footnote{While KNL supports 4 threads/core, only 1 is considered here as performance may degrade over shared resources.}.
    \end{paragraph}
    \begin{paragraph}{}
      The matrix multiplication functions provided with the Intel Math Kernel Library (MKL) will be consulted as a reference for empirically
      achievable single-node performance either without GPU acceleration or in the case of KNL nodes while the matrix multiplication sample provided
      by the CUDA toolkit utilizing CUBLAS functions will be consulted as a reference for empirically achievable single-node performance
      with GPU acceleration.
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Hypotheses}
    I predict that the accelerated OpenMP solution will outperform the MPI solution for smaller matrix sizes until
    reaching an inflection point where the contention between resources within the node is a greater bottleneck than
    the communication overhead across multiple nodes. This inflection point will largely depend on the architecture
    and layout of the accelerator used, i.e., the dimensions of the various hierarchical groupings of its compute resources.
  \end{subsection}
\end{section}

\begin{section}{Validation}
  \begin{paragraph}{}
    Before commencing on the actual implementation of the matrix multiplication programs, it was determined that the design of
    the validation trials would take first priority so as to catch errors in implementation earlier rather than later.
    The validation mechanism itself needed to operate under several exacting constraints. It has been already mentioned
    that the $m \times n$ product matrix $C$ in validation mode would need to house a unique value in each cell once calculations had finished.
    This was to ensure that every dot product of each row in $m \times q$ matrix $A$ and each column in $q \times n$ matrix $B$ did not suffer corruption
    from any other dot products or other implementation errors by providing a quick reference that could be used as a sanity check.
  \end{paragraph}
  \begin{paragraph}{}
    Alacrity in the validation implementation was key. If the validation required additional extended calculations to replicate the results serially,
    the time to find and correct errors could become prohibitively slow, especially as the matrix dimensions $m$, $q$, and $n$ increased.
    Ideally, the validation would be a constant time function $f$ based on the row and column index of the cell in question, i.e.,
    $C_{i,j} = f(i,j)$.
  \end{paragraph}
  \begin{paragraph}{}
    The under-the-hood 1D row-major ordering and indexing of 2D matrices in the C programming language provided the requisite inspiration.
    The cell of every 2D matrix in C is accessed implicitly by the following formula: $iq +j$, where $i$ is the row index,
    $j$ is the column index, and $n$ is the number of columns\autocite[][113]{KnR}. Initial attempts sough to find some function $f(i,j) = \displaystyle\sum_{k=1}^{N} A_{i,k} \cdot B_{k,j}$
    such that the matrix $A$ independently provided the $i$ component while the matrix $B$ independently provided the $j$ component.
  \end{paragraph}
  \begin{paragraph}{}
    After wrestling with the algebra for some time, a simpler solution was devised. Each cell of the matrix $A$
    would be populated with its row index $i$. Each cell of the matrix $B$ would be populated with the constant $1$.
    Each cell of the product matrix $C$ would be pre-populated with its column index $j$ instead of the customary constant $0$.
    In this way, the equation $C = A \times B$ became $C = A \times B + C_{0}$ where performance trials would start with
    $C_{0} = \textbf{0}$ and validation trials would start with $C_{0} = [\textbf{0}\ \textbf{1}\ \ldots\ \textbf{m}]$.
  \end{paragraph}
  \begin{paragraph}{}
    While this satisfies the desired formula $in + j$ per cell, it does not provide a checksum against column drift
    since $B$ is simply a homogeneous field of ones. The basic principle could be improved as follows. Let each cell of the matrix
    $A$ be populated with the value $i + 1$. Let each cell of the matrix $B$ be populated with the value $j$. If a basis
    of $C_{0}$ = \textbf{0} is assumed, each cell of the product matrix $C$ will contain the value $q(i + 1)j$, which expands to
    $qij + qj$. By setting the basis matrix $C_{0}$ so that each cell contains $-((q-1)j + (j-1)iq)$ , each cell of the product matrix $C$
    will then contain the desired value $iq + j$. This is a relatively safe method for obtaining the desired value since there are no
    division operations required.
  \end{paragraph}
  \begin{paragraph}{}
    The problem with this latest refinement and even all the previous refinements is that the resultant values did not fit within the range of precise integral
    values available in single-precision floating point, i.e., [$-2^{24}$,$2^{24}$]\autocite{SPIntLimit}, given the target matrix sizes. The validation
    method needed to be able to precisely support values as high as $16383 \times 16383 + 16382 \times 16383 \times 16384$, or something on the order
    of $2^{42}$. Several attempts to compress the range by reducing $i$ and $j$ failed to produce the desired results and only complicated the requisite
    basis matrix $C_{0}$ with no guarantee of precision fidelity.
  \end{paragraph}
  \begin{paragraph}{}
    Furthermore, the previous equation made an untenable assumption in view of the proposed target matrix sizes. The inner dimension $q$ is not
    necessarily equal to the final column dimension $n$ of the matrix $C$. Such an invariant only holds if the matrices $A$ and $B$ are square.
  \end{paragraph}
\end{section}

\begin{section}{Empirical Rooflines}
  In order to get a gauge for the current state of the art as regards matrix multiplication, it was necessary to develop a couple of test harnesses.
  These harnesses could provide a reasonable estimate of what performance and efficiency would be achievable within the bounds of the peak theoretical
  rooflines. Because of the difference in architectures between the scale-up and scale-out cases, at least two harnesses would need to be developed.
  \begin{subsection}{Scale-Up}
    \begin{paragraph}{}
      For the GPU offloading or scale-up case, the NVIDIA CUBLAS library was selected as the empirical roofline model. It is assumed that the development
      staff at NVIDIA has a more intimate knowledge of the NVIDIA P100 and NVIDIA K80 platforms than the general public and therefore should be better
      equipped to maximize resource utilization for those architectures.
    \end{paragraph}
    \begin{paragraph}{}
      The \texttt{matrixMulCUBLAS} sample source provided with CUDA Toolkit 8.0 served as the basis for the CUBLAS test harness. Some minor modifications
      were made to the command-line arguments to support a wider array of target matrix sizes as the original source hardcoded the dimensions based on a single
      multiplier. The \texttt{Makefile} was also modified to make the entire harness self-contained, including some header files common to most of the sample
      programs provided with the CUDA toolkit.
    \end{paragraph}
    \begin{paragraph}{}
      To run the scale-up harness, one would type the following command:
      \begin{subparagraph}{}
        \begin{alltt}
          ./matrixMulCUBLAS m=\(m\) q=\(q\) n=\(n\) validation=[0|1]
        \end{alltt}
      \end{subparagraph}
    \end{paragraph}
    \begin{paragraph}{}
      The original source validated the CUBLAS results by calculating a reference solution on the host. Given the target matrix sizes involved, this could
      have become prohibitively expensive in terms of time cost, especially in early stage development on the author's local laptop machine.
      This validation was made optional through the \texttt{validation} command-line argument. The default is to forgo validation, but validation of a sort
      can be re-enabled by adding \texttt{validation=1} to the command-line call. At present, the only validation that is done is to seed the matrices $A$
      and $B$ as homogeneous fields of ones, i.e., $A$ = \textbf{1} and $B$ = \textbf{1}. The validation simply checks that every cell of the product matrix
      $C$ is set to $q$.
    \end{paragraph}
    \begin{paragraph}{}
      The arguments \texttt{m}, \texttt{q}, and \texttt{n} can be any arbitrary integer, and no check is currently done for alignment against the CUDA card
      except what may have existed in the original source. Since the target matrix sizes for the experiment are limited to powers of two (2), this is deemed
      an acceptable risk.
    \end{paragraph}
    \begin{paragraph}{}
      The output of the program has a fair amount of helpful human-readable information that is piped to \texttt{stderr}. Most of the \texttt{stderr} output
      has its origins in the original source provided with the CUDA toolkit. On the other hand, output to \texttt{stdout} is reserved for machine-readable CSV text. 
      By convention, the output consists of the following columns:
      \begin{enumerate}
      \item{$m$, the row dimension of $A$ and $C$}
      \item{$q$, the column dimension of $A$ and the row dimension of $B$}
      \item{$n$, the column dimension of $B$ and $C$}
      \item{$r$, the achieved rate (gigaflops)}
      \item{$s$, the number of iterations}
      \item{$t_{iter}$, the average time spent per iteration (ms)}
      \item{$r_{iter}$, the achieved rate per iteration (flops)}
      \item{$t_{comm}$, the time spent in host-to-device or device-to-host transfers (ms)}
      \end{enumerate}
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Scale-Out}
    \begin{paragraph}{}
      For the unaccelerated MPI or scale-out case, the Intel MKL libraries and their concomitant BLAS3 functions were selected as the empirical roofline model.
      Since all of the scale-out cases will be run on nodes with Intel processors, it is assumed that the development staff at Intel has a more intimate knowledge
      of the processors involved than the general public and therefore should be better equipped to maximize resource utilization on these architectures.
    \end{paragraph}
    \begin{paragraph}{}
      There is a slight difference in the methodology involved here from that employed in the scale-up case. Whereas the scale-up case looked at resource utilization
      holistically with regard to the accelerator, the harness for the scale-out case will evaluate the single-core performance of the MKL BLAS3 function
      \texttt{cblas\_sgemm}\autocite{MKLcblas\_sgemm}, and the roofline will be extrapolated along a strong-scaling model so that a scale-out case running with $p$
      processors will be measured against the single-core rate multiplied by $p$ to determine its efficiency relative to MKL.
    \end{paragraph}
    \begin{paragraph}{}
      The sample code provided by Intel for \texttt{cblas\_sgemm}\autocite{MKLcblas\_sgemmx} was consulted as a reference for the scale-out single-core
      basis harness to validate correctness of the implementation, particularly the call to \texttt{cblas\_sgemm}, but the API documentation served as the
      primary resource that informed the implementation not only of the MKL library usage but the devising of command-line options as well. In fact, the
      command-line options of the scale-up harness were altered to match those of the scale-out harness for legibility and ease of batch scripting later.
    \end{paragraph}
    \begin{paragraph}{}
      To run the scale-out harness, one would type the following command:
      \begin{subparagraph}{}
        \begin{alltt}
          ./mkl\_cblas\_sgemm -m \(m\) -q \(q\) -n \(n\) validation=[0|1]
        \end{alltt}
      \end{subparagraph}
    \end{paragraph}
    \begin{paragraph}{}
      The arguments provided to \texttt{-m}, \texttt{-q}, and \texttt{-n} can be any arbitrary integer.
    \end{paragraph}
    \begin{paragraph}{}
      Output to \texttt{stdout} is reserved for machine-readable CSV text. By convention, the output consists of the following columns:
      \begin{enumerate}
      \item{$m$, the row dimension of $A$ and $C$}
      \item{$q$, the column dimension of $A$ and the row dimension of $B$}
      \item{$n$, the column dimension of $B$ and $C$}
      \item{$r$, the achieved rate (gigaflops)}
      \item{$s$, the number of iterations}
      \item{$t_{iter}$, the average time spent per iteration (ms)}
      \item{$r_{iter}$, the achieved rate per iteration (flops)}
      \end{enumerate}
    \end{paragraph}
  \end{subsection}
\end{section}

\begin{section}{Implementation}
  \begin{subsection}{Scale-Up}
    \begin{paragraph}{}
      Initial development on the scale-up implementation of Cannon's algorithm began with OpenMP. Some difficulties were encountered with regard
      to the offset math required for setting up a non-square 2D grid which delayed development of the parallelization. Once the problem was
      reduced to the square target matrix sizes, it was realized that part of the problem was the failure to enforce the rigidity of the grid dimensions
      and coupled with a misreading of the algorithm description with regard to the initial shearing of phase 1.
    \end{paragraph}
    \begin{paragraph}{}
      An additional hurdle was encountered when it became clear that OpenMP GPU offloading was not enabled with either gcc or icc on the intended
      BRIDGES computing cluster. Reading the GPU user guide for BRIDGES\autocite{BridgesGPUGuide} made it clear that the preferred and supported means of \texttt{\#pragma}-centric
      acceleration was OpenACC with the PGI compiler. A quick perusal of the GCC documentation for OpenACC support\autocite{gccOpenACC} revealed a series
      of known issues that made GCC infeasible for use, even with the latest 7.2 release. Namely, the lack of support for host fallback and nested
      parallel loop support eliminated GCC from consideration.
    \end{paragraph}
    \begin{paragraph}
      As it stands right now, the OpenACC implementation is functional, but not at all performant. There are some additional modifications to be made
      to take advantage of acceleration, primarily in the block multiplication phase. Presently, the pgcc compiler is only adding serial accelerated
      loops because of loop variable dependencies.
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Scale-Out}
    \begin{paragraph}{}
      Development of the scale-out implementation borrowed the core of the OpenACC implementation from the scale-up implementation and was ported
      to use the MPI library functions. This port employed the RMA communication mechanism with Post-Start-Complete-Wait synchronization since
      the cyclic rotation between MPI processes must be marshalled in such a way that the present data is not overwritten.
    \end{paragraph}
    \begin{paragraph}{Array Distribution}
      During program startup, the root process $P_{0,0}$ fills the arrays $A$, $B$, and $C$ and issues a \texttt{MPI\_Win\_post} call to \texttt{MPI\_COMM\_WORLD}
      to notify the other processes and itself that the arrays are ready for retrieval. Each process $P_{x,y}$ issues a \texttt{MPI\_Get} request
      to the root process $P_{0,0}$ for the properly sheared 2D subarrays $A_{x,(y+x)\ mod\ c}$ and $B_{(x+y)\ mod\ b, y}$ along with the stationary
      2D subarray $C_{x,y}$. These are stored in individual process memory and opened via another RMA window so that the requisite neighbor processes
      can access them during the cyclic rotation phases.
    \end{paragraph}
    \begin{paragraph}{Cyclic Rotation}
      Within each process, the memory allocated for the $A$ and $B$ subarrays is actually twice that required. This is to enable whole-block copying
      between neighbor processes. The additional space serves as a send buffer during cyclic rotation and prevents corruption. Each process $P_{x,y}$
      copies the entirety of the $A$ ($B$) submatrix and then issues a \texttt{MPI\_Win\_post} call for the corresponding window to a group comprised of
      only its east (south) neighbor so that it can receive the other's copy of the corresponding submatrix. Immediately after the \texttt{MPI\_Post}
      calls are sent, the process $P_{x,y}$ then issues a \texttt{MPI\_Win\_Start} call to its west (north) neighbor so that it can send its prior submatrix
      $A$ ($B$) from the ghost block. Once that neighbor has posted its readiness, the process $P_{x,y}$ issues a \texttt{MPI\_Put} to deliver the data.
      The transaction is finalized with a call to \texttt{MPI\_Win\_complete} on the corresponding window, and then the process issues a \texttt{MPI\_Win\_wait}
      to block until it has assurance that its supplier has finished the incoming transaction before proceeding to the multiplication phase.
    \end{paragraph}
    \begin{paragraph}{Local Multiplication}
      The local multiplication is implemented as a simple \emph{ikj} block multiplication. Some attempts have been made to optimize it
      with OpenMP, but the parallelizations have turned out to be worse than the serial in local development tests.
      After each local multiplication is finished, the process $P\_{x,y}$ loops until it has processed the block row $x$ and block column $y$
      at most once.
    \end{paragraph}
    \begin{paragraph}{Collection}
      When all the work has finished, the various processes report back to the root process via individual \texttt{MPI\_Put} calls to load balance
      the communication. It remains to be seen whether a more complex call like \texttt{MPI\_Gather} can be used to optimize the implementation.
    \end{paragraph}
    \begin{paragraph}
    \end{paragraph}
  \end{subsection}
\end{section}
\printbibliography

\end{document}
