\documentclass{article}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{alltt}
\usepackage[utf8]{luainputenc}
\usepackage[bibencoding=utf8,backend=biber]{biblatex}
\addbibresource{COSC-6365-MichaelYantosca-FinalProjectReport.bib}
\pagestyle{fancy}
\fancyhf{}
\rhead{COSC-6365 Final Project Proposal}
\lhead{Michael Yantosca}
\rfoot{\thepage}

\begin{document}
\begin{section}{Abstract}
  To explore the relative merits between single-node computer power in depth versus multi-node compute power in breadth,
  I propose an experiment comparing the performance and efficiency of Cannon's algorithm\autocites{Lecture17Slides,GuptaSadayappan} for the matrix multiplication
  problem $C = A \times B$ between a single-node OpenMP implementation utilizing accelerators and a multi-node MPI
  implementation spread across a cluster.
  \begin{subsection}{Data}
    For the sake of time, I will only consider the $C$-stationary case for a limited number of matrix sizes
    (256x256, 1024x1024, 4096x4096, 16384x16384, 256x1024, 256x4096, 256x16384, 16384x256, 16384x256, 4096x256, 1024x256).
    The matrices will be filled with random numbers for the performance trials. Validation trials will be made
    through specially devised matrices $A$ and $B$ such that each cell of the product matrix $C$ will have a unique value.
  \end{subsection}
  \begin{subsection}{Resources}
    The OpenMP trials will be executed on the GPU nodes on BRIDGES\footnote{If the GPU nodes on BRIDGES do not support
    OpenMP offloading to the P100 and K80 GPUs, the acceleration will be done via OpenACC, or, as a last resort, CUDA.}
    and/or the KNL nodes on STAMPEDE2, whereas the MPI trials will be executed on the appropriate non-accelerated nodes
    on BRIDGES.
  \end{subsection}
  \begin{subsection}{Efficiency}
    The efficiency of the MPI trials will be gauged against a strong-scaling roofline model based on the physical
    characteristics of a single non-GPU node on BRIDGES. The processor in this case is the 28-core, 2.30 GHz Intel E5-2695,
    which is theoretically capable of 2.30 GHz * 28 cores * 4 SIMD instructions/cycle (AVX256) = 257.6 GFLOPs/s/node.
    \begin{paragraph}{}
      The efficiency of the OpenMP trials will be gauged against a roofline model based on the physical characteristics of the accelerator.
      The NVIDIA P100 is theoretically capable of 9.3 SP TFLOPs/s/card\autocite{P100Datasheet}, whereas the NVIDIA K80 is theoretically capable of
      8.74 TFLOPs/s/card\autocite{AnandtechK80}. In the event that KNL nodes on STAMPEDE2 are utilized, the theoretical roofline will
      be considered as 1.4 GHz * 68 cores * 8 SIMD instructions/cycle (AVX512) = 761.6 GFLOPs/s/node\autocite{Stampede2UserGuide}\footnote{While KNL supports 4 threads/core, only 1 is considered here as performance may degrade over shared resources.}.
    \end{paragraph}
    \begin{paragraph}{}
      The matrix multiplication functions provided with the Intel Math Kernel Library (MKL) will be consulted as a reference for empirically
      achievable single-node performance either without GPU acceleration or in the case of KNL nodes while the matrix multiplication sample provided
      by the CUDA toolkit utilizing CUBLAS functions will be consulted as a reference for empirically achievable single-node performance
      with GPU acceleration.
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Hypotheses}
    I predict that the accelerated OpenMP solution will outperform the MPI solution for smaller matrix sizes until
    reaching an inflection point where the contention between resources within the node is a greater bottleneck than
    the communication overhead across multiple nodes. This inflection point will largely depend on the architecture
    and layout of the accelerator used, i.e., the dimensions of the various hierarchical groupings of its compute resources.
  \end{subsection}
\end{section}

\begin{section}{Validation}
  \begin{paragraph}{}
    Before commencing on the actual implementation of the matrix multiplication programs, it was determined that the design of
    the validation trials would take first priority so as to catch errors in implementation earlier rather than later.
    The validation mechanism itself needed to operate under several exacting constraints. It has been already mentioned
    that the $m \times n$ product matrix $C$ in validation mode would need to house a unique value in each cell once calculations had finished.
    This was to ensure that every dot product of each row in $m \times q$ matrix $A$ and each column in $q \times n$ matrix $B$ did not suffer corruption
    from any other dot products or other implementation errors by providing a quick reference that could be used as a sanity check.
  \end{paragraph}
  \begin{paragraph}{}
    Alacrity in the validation implementation was key. If the validation required additional extended calculations to replicate the results serially,
    the time to find and correct errors could become prohibitively slow, especially as the matrix dimensions $m$, $q$, and $n$ increased.
    Ideally, the validation would be a constant time function $f$ based on the row and column index of the cell in question, i.e.,
    $C_{i,j} = f(i,j)$.
  \end{paragraph}
  \begin{paragraph}{}
    The under-the-hood 1D row-major ordering and indexing of 2D matrices in the C programming language provided the requisite inspiration.
    The cell of every 2D matrix in C is accessed implicitly by the following formula: $iq +j$, where $i$ is the row index,
    $j$ is the column index, and $n$ is the number of columns\autocite[][113]{KnR}. Initial attempts sough to find some function $f(i,j) = \displaystyle\sum_{k=1}^{N} A_{i,k} \cdot B_{k,j}$
    such that the matrix $A$ independently provided the $i$ component while the matrix $B$ independently provided the $j$ component.
  \end{paragraph}
  \begin{paragraph}{}
    After wrestling with the algebra for some time, a simpler solution was devised. Each cell of the matrix $A$
    would be populated with its row index $i$. Each cell of the matrix $B$ would be populated with the constant $1$.
    Each cell of the product matrix $C$ would be pre-populated with its column index $j$ instead of the customary constant $0$.
    In this way, the equation $C = A \times B$ became $C = A \times B + C_{0}$ where performance trials would start with
    $C_{0} = \textbf{0}$ and validation trials would start with $C_{0} = [\textbf{0}\ \textbf{1}\ \ldots\ \textbf{m}]$.
  \end{paragraph}
  \begin{paragraph}{}
    While this satisfies the desired formula $in + j$ per cell, it does not provide a checksum against column drift
    since $B$ is simply a homogeneous field of ones. The basic principle could be improved as follows. Let each cell of the matrix
    $A$ be populated with the value $i + 1$. Let each cell of the matrix $B$ be populated with the value $j$. If a basis
    of $C_{0}$ = \textbf{0} is assumed, each cell of the product matrix $C$ will contain the value $q(i + 1)j$, which expands to
    $qij + qj$. By setting the basis matrix $C_{0}$ so that each cell contains $-((q-1)j + (j-1)iq)$ , each cell of the product matrix $C$
    will then contain the desired value $iq + j$. This is a relatively safe method for obtaining the desired value since there are no
    division operations required.
  \end{paragraph}
  \begin{paragraph}{}
    The problem with this latest refinement and even all the previous refinements is that the resultant values did not fit within the range of precise integral
    values available in single-precision floating point, i.e., [$-2^{24}$,$2^{24}$]\autocite{SPIntLimit}, given the target matrix sizes. The validation
    method needed to be able to precisely support values as high as $16383 \times 16383 + 16382 \times 16383 \times 16384$, or something on the order
    of $2^{42}$. Several attempts to compress the range by reducing $i$ and $j$ failed to produce the desired results and only complicated the requisite
    basis matrix $C_{0}$ with no guarantee of precision fidelity.
  \end{paragraph}
  \begin{paragraph}{}
    Furthermore, the previous equation made an untenable assumption in view of the proposed target matrix sizes. The inner dimension $q$ is not
    necessarily equal to the final column dimension $n$ of the matrix $C$. Such an invariant only holds if the matrices $A$ and $B$ are square.
  \end{paragraph}
\end{section}

\begin{section}{Empirical Rooflines}
  In order to get a gauge for the current state of the art as regards matrix multiplication, it was necessary to develop a couple of test harnesses.
  These harnesses could provide a reasonable estimate of what performance and efficiency would be achievable within the bounds of the peak theoretical
  rooflines. Because of the difference in architectures between the scale-up and scale-out cases, at least two harnesses would need to be developed.
  \begin{subsection}{Scale-Up}
    \begin{paragraph}{}
      For the GPU offloading or scale-up case, the NVIDIA CUBLAS library was selected as the empirical roofline model. It is assumed that the development
      staff at NVIDIA has a more intimate knowledge of the NVIDIA P100 and NVIDIA K80 platforms than the general public and therefore should be better
      equipped to maximize resource utilization for those architectures.
    \end{paragraph}
    \begin{paragraph}{}
      The \texttt{matrixMulCUBLAS} sample source provided with CUDA Toolkit 8.0 served as the basis for the CUBLAS test harness. Some minor modifications
      were made to the command-line arguments to support a wider array of target matrix sizes as the original source hardcoded the dimensions based on a single
      multiplier. The \texttt{Makefile} was also modified to make the entire harness self-contained, including some header files common to most of the sample
      programs provided with the CUDA toolkit.
    \end{paragraph}
    \begin{paragraph}{}
      To run the scale-up harness, one would type the following command:
      \begin{subparagraph}{}
        \begin{alltt}
          ./matrixMulCUBLAS m=\(m\) q=\(q\) n=\(n\) validation=[0|1]
        \end{alltt}
      \end{subparagraph}
    \end{paragraph}
    \begin{paragraph}{}
      The original source validated the CUBLAS results by calculating a reference solution on the host. Given the target matrix sizes involved, this could
      have become prohibitively expensive in terms of time cost, especially in early stage development on the author's local laptop machine.
      This validation was made optional through the \texttt{validation} command-line argument. The default is to forgo validation, but validation of a sort
      can be re-enabled by adding \texttt{validation=1} to the command-line call. At present, the only validation that is done is to seed the matrices $A$
      and $B$ as homogeneous fields of ones, i.e., $A$ = \textbf{1} and $B$ = \textbf{1}. The validation simply checks that every cell of the product matrix
      $C$ is set to $q$.
    \end{paragraph}
    \begin{paragraph}{}
      The arguments \texttt{m}, \texttt{q}, and \texttt{n} can be any arbitrary integer, and no check is currently done for alignment against the CUDA card
      except what may have existed in the original source. Since the target matrix sizes for the experiment are limited to powers of two (2), this is deemed
      an acceptable risk.
    \end{paragraph}
    \begin{paragraph}{}
      The output of the program has a fair amount of helpful human-readable information that is piped to \texttt{stderr}. Most of the \texttt{stderr} output
      has its origins in the original source provided with the CUDA toolkit. On the other hand, output to \texttt{stdout} is reserved for machine-readable CSV text.
      By convention, the output consists of the following columns:
      \begin{enumerate}
      \item{$m$, the row dimension of $A$ and $C$}
      \item{$q$, the column dimension of $A$ and the row dimension of $B$}
      \item{$n$, the column dimension of $B$ and $C$}
      \item{$r$, the achieved rate (gigaflops)}
      \item{$s$, the number of iterations}
      \item{$t_{iter}$, the average time spent per iteration (ms)}
      \item{$r_{iter}$, the achieved rate per iteration (flops)}
      \item{$t_{comm}$, the time spent in host-to-device or device-to-host transfers (ms)}
      \end{enumerate}
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Scale-Out}
    \begin{paragraph}{}
      For the unaccelerated MPI or scale-out case, the Intel MKL libraries and their concomitant BLAS3 functions were selected as the empirical roofline model.
      Since all of the scale-out cases will be run on nodes with Intel processors, it is assumed that the development staff at Intel has a more intimate knowledge
      of the processors involved than the general public and therefore should be better equipped to maximize resource utilization on these architectures.
    \end{paragraph}
    \begin{paragraph}{}
      There is a slight difference in the methodology involved here from that employed in the scale-up case. Whereas the scale-up case looked at resource utilization
      holistically with regard to the accelerator, the harness for the scale-out case will evaluate the single-core performance of the MKL BLAS3 function
      \texttt{cblas\_sgemm}\autocite{MKLcblas\_sgemm}, and the roofline will be extrapolated along a strong-scaling model so that a scale-out case running with $p$
      processors will be measured against the single-core rate multiplied by $p$ to determine its efficiency relative to MKL.
    \end{paragraph}
    \begin{paragraph}{}
      The sample code provided by Intel for \texttt{cblas\_sgemm}\autocite{MKLcblas\_sgemmx} was consulted as a reference for the scale-out single-core
      basis harness to validate correctness of the implementation, particularly the call to \texttt{cblas\_sgemm}, but the API documentation served as the
      primary resource that informed the implementation not only of the MKL library usage but the devising of command-line options as well. In fact, the
      command-line options of the scale-up harness were altered to match those of the scale-out harness for legibility and ease of batch scripting later.
    \end{paragraph}
    \begin{paragraph}{}
      To run the scale-out harness, one would type the following command:
      \begin{subparagraph}{}
        \begin{alltt}
          ./mkl\_cblas\_sgemm -m \(m\) -q \(q\) -n \(n\) validation=[0|1]
        \end{alltt}
      \end{subparagraph}
    \end{paragraph}
    \begin{paragraph}{}
      The arguments provided to \texttt{-m}, \texttt{-q}, and \texttt{-n} can be any arbitrary integer.
    \end{paragraph}
    \begin{paragraph}{}
      Output to \texttt{stdout} is reserved for machine-readable CSV text. By convention, the output consists of the following columns:
      \begin{enumerate}
      \item{$m$, the row dimension of $A$ and $C$}
      \item{$q$, the column dimension of $A$ and the row dimension of $B$}
      \item{$n$, the column dimension of $B$ and $C$}
      \item{$r$, the achieved rate (gigaflops)}
      \item{$s$, the number of iterations}
      \item{$t_{iter}$, the average time spent per iteration (ms)}
      \item{$r_{iter}$, the achieved rate per iteration (flops)}
      \end{enumerate}
    \end{paragraph}
  \end{subsection}
\end{section}

\begin{section}{Implementation}
  \begin{subsection}{Scale-Up}
    \begin{paragraph}{}
      Initial development on the scale-up implementation of Cannon's algorithm began with OpenMP. Some difficulties were encountered with regard
      to the offset math required for setting up a non-square 2D grid which delayed development of the parallelization. Once the problem was
      reduced to the square target matrix sizes, it was realized that part of the problem was the failure to enforce the rigidity of the grid dimensions
      and coupled with a misreading of the algorithm description with regard to the initial shearing of phase 1.
    \end{paragraph}
    \begin{paragraph}{}
      An additional hurdle was encountered when it became clear that OpenMP GPU offloading was not enabled with either gcc or icc on the intended
      BRIDGES computing cluster. Reading the GPU user guide for BRIDGES\autocite{BridgesGPUGuide} made it clear that the preferred and supported means of \texttt{\#pragma}-centric
      acceleration was OpenACC with the PGI compiler. A quick perusal of the GCC documentation for OpenACC support\autocite{gccOpenACC} revealed a series
      of known issues that made GCC infeasible for use, even with the latest 7.2 release. Namely, the lack of support for host fallback and nested
      parallel loop support eliminated GCC from consideration.
    \end{paragraph}
    \begin{paragraph}{}
      As it stands right now, the OpenACC implementation is functional, but not at all performant. There are some additional modifications to be made
      to take advantage of acceleration, primarily in the block multiplication phase. Presently, the pgcc compiler is only adding serial accelerated
      loops because of loop variable dependencies.
    \end{paragraph}
    \begin{paragraph}{}
      It was determined that a more rigorous examination of the parallelization would be required. The ordering of the loop variables would likely
      need changing, and a better understanding of OpenACC directives was sought.

      After reviewing some tutorials on OpenACC parallelization techniques, some experiments with loop ordering were attempted and explicit
      \texttt{\#pragma acc loop independent} directives were added in the more deeply nested loops. The alleged conflicts with \texttt{dA} and
      \texttt{dB} disappeared, but the outermost loops were still sequential. It was not until violating one of the most widely disseminated
      best practices, namely the avoidance of pointer arithmetic, that all the loops became parallelizable. After a quick verification check
      that the results were valid for a small case, the smallest target matrix size was tested under a nvprof wrapper with the following results:
      \begin{alltt}
        {\footnotesize
          michael@aphelion ~/cosc6365/final $ nvprof ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          ==12882== NVPROF is profiling process 12882, command: ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          16,256,256,256,1,33554432.00,0.00,1319.012,82.073
          ==12882== Profiling application: ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          ==12882== Profiling result:
          Time(%)      Time     Calls       Avg       Min       Max  Name
          99.00%  1.31888s         4  329.72ms  326.65ms  337.61ms  main_356_gpu
          0.61%  8.1765ms      2049  3.9900us  3.6800us  169.38us  [CUDA memcpy HtoD]
          0.37%  4.9208ms      5144     956ns     896ns  10.432us  [CUDA memcpy DtoD]
          0.01%  160.00us         1  160.00us  160.00us  160.00us  [CUDA memcpy DtoH]

          ==12882== API calls:
          Time(%)      Time     Calls       Avg       Min       Max  Name
          85.39%  1.33983s      2054  652.30us  4.1210us  337.62ms  cuStreamSynchronize
          6.29%  98.630ms         1  98.630ms  98.630ms  98.630ms  cuDevicePrimaryCtxRetain
          4.16%  65.316ms         1  65.316ms  65.316ms  65.316ms  cuDevicePrimaryCtxRelease
          1.86%  29.147ms      5144  5.6660us  5.0490us  325.27us  cuMemcpyDtoDAsync
          1.03%  16.215ms         1  16.215ms  16.215ms  16.215ms  cuMemHostAlloc
          0.59%  9.2858ms         1  9.2858ms  9.2858ms  9.2858ms  cuMemFreeHost
          0.41%  6.4971ms      2049  3.1700us  2.9330us  23.480us  cuMemcpyHtoDAsync
          0.17%  2.7347ms     10288     265ns     211ns  2.7000us  cuPointerGetAttributes
          0.04%  695.98us         4  174.00us  3.9990us  241.37us  cuMemAlloc
          0.03%  409.59us         1  409.59us  409.59us  409.59us  cuMemAllocHost
          0.01%  149.14us         1  149.14us  149.14us  149.14us  cuModuleLoadData
          0.00%  32.707us         4  8.1760us  6.7910us  10.511us  cuLaunchKernel
          0.00%  21.719us         1  21.719us  21.719us  21.719us  cuStreamCreate
          0.00%  20.038us         1  20.038us  20.038us  20.038us  cuMemcpyDtoHAsync
          0.00%  5.6230us         2  2.8110us     454ns  5.1690us  cuEventCreate
          0.00%  2.5610us         1  2.5610us  2.5610us  2.5610us  cuEventRecord
          0.00%  2.2920us         3     764ns     163ns  1.7650us  cuDeviceGetCount
          0.00%  1.8190us         3     606ns     293ns  1.0710us  cuCtxSetCurrent
          0.00%  1.3030us         3     434ns     272ns     731ns  cuDeviceGet
          0.00%  1.1740us         1  1.1740us  1.1740us  1.1740us  cuModuleGetFunction
          0.00%  1.1310us         4     282ns     250ns     338ns  cuDeviceGetAttribute
          0.00%  1.0130us         1  1.0130us  1.0130us  1.0130us  cuEventSynchronize
          0.00%     317ns         1     317ns     317ns     317ns  cuMemFree
          0.00%     292ns         1     292ns     292ns     292ns  cuCtxGetCurrent
          0.00%     289ns         1     289ns     289ns     289ns  cuDeviceComputeCapability

          ==12882== OpenACC (excl):
          Time(%)      Time     Calls       Avg       Min       Max  Name
          91.10%  1.31913s         5  263.83ms  210.04us  337.63ms  acc_wait@byteswap.h:356
          6.87%  99.469ms         1  99.469ms  99.469ms  99.469ms  acc_device_init
          1.50%  21.778ms      2049  10.628us  4.7710us  171.59us  acc_wait
          0.52%  7.5036ms      2049  3.6620us  3.4070us  29.945us  acc_enqueue_upload
          0.00%  44.429us         4  11.107us  9.2100us  14.338us  acc_enqueue_launch@byteswap.h:356 (main_356_gpu)
          0.00%  34.080us         1  34.080us  34.080us  34.080us  acc_enqueue_download@byteswap.h:356
          0.00%  29.343us         4  7.3350us  5.5510us  12.177us  acc_compute_construct@byteswap.h:356
          0.00%       0ns         3       0ns       0ns       0ns  acc_delete@(OpenACC API):1
          0.00%       0ns         3       0ns       0ns       0ns  acc_alloc@(OpenACC API):1
        }
      \end{alltt}
    \end{paragraph}
    \begin{paragraph}{}
      The modifications garnered roughly 4 times better performance than prior attempts. As evidenced by the nvprof output,
      the bulk of time was being spent in \texttt{acc\_wait}, an implicit synchronization emplaced to protect the contents
      of \texttt{dC}, but at least the multiplication was finally out of the hands of the host processor. The question then became
      how to reduce the rather significant amount of context swapping.
    \end{paragraph}
    \begin{paragraph}{}
      A quick experiment in swapping the \texttt{\#pragma acc parallel} \texttt{\#pragma acc kernels} immediately doubled the performance.
      Another brief experiment in reverting to $ijk$ multiplication and leveraging the \texttt{\#pragma acc loop reduction} directive\autocite[][2]{WolfePGI} revealed
      a disappointing truth: the program ran fastest when $p$ was set to 1, i.e., no communication between blocks. It is becoming clear
      that the attempt to save on memory costs by allocating as one large block is defeating the benefits by block allocation since the
      data will not be block-local for the streaming processors if $b$ and $c$ are to be kernel dimensions.
    \end{paragraph}
    \begin{paragraph}{}
      Further experimentation with precalculating offsets yielded promising results. By moving to an $ixykj$ loop model and precalculating
      the parts of the offsets pertaining to $i$, $x$, and $y$ in the $y$ loop, performance was greatly improved.
      \begin{alltt}
        {\footnotesize
          michael@aphelion ~/cosc6365/final $ nvprof ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          ==8811== NVPROF is profiling process 8811, command: ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          16,256,256,256,1,33554432.00,0.00,136.267,81.453
          ==8811== Profiling application: ./cannon-su-acc -p 16 -m 256 -q 256 -n 256
          ==8811== Profiling result:
          Time(%)      Time     Calls       Avg       Min       Max  Name
          91.06%  136.17ms         4  34.042ms  31.990ms  35.053ms  main_344_gpu
          5.38%  8.0485ms      2049  3.9280us  3.6800us  169.38us  [CUDA memcpy HtoD]
          3.45%  5.1647ms      5144  1.0040us     896ns  14.912us  [CUDA memcpy DtoD]
          0.11%  160.80us         1  160.80us  160.80us  160.80us  [CUDA memcpy DtoH]

          ==8811== API calls:
          Time(%)      Time     Calls       Avg       Min       Max  Name
          40.09%  156.51ms      2054  76.199us  3.9950us  35.060ms  cuStreamSynchronize
          27.05%  105.60ms         1  105.60ms  105.60ms  105.60ms  cuDevicePrimaryCtxRetain
          16.28%  63.552ms         1  63.552ms  63.552ms  63.552ms  cuDevicePrimaryCtxRelease
          7.37%  28.755ms      5144  5.5900us  5.0030us  363.46us  cuMemcpyDtoDAsync
          4.05%  15.824ms         1  15.824ms  15.824ms  15.824ms  cuMemHostAlloc
          2.42%  9.4366ms         1  9.4366ms  9.4366ms  9.4366ms  cuMemFreeHost
          1.69%  6.6098ms      2049  3.2250us  2.9770us  23.551us  cuMemcpyHtoDAsync
          0.70%  2.7276ms     10288     265ns     208ns  13.388us  cuPointerGetAttributes
          0.18%  719.74us         4  179.93us  4.3360us  264.30us  cuMemAlloc
          0.10%  405.40us         1  405.40us  405.40us  405.40us  cuMemAllocHost
          0.04%  158.18us         1  158.18us  158.18us  158.18us  cuModuleLoadData
          0.01%  44.311us         1  44.311us  44.311us  44.311us  cuStreamCreate
          0.01%  26.994us         4  6.7480us  5.5120us  9.2010us  cuLaunchKernel
          0.01%  19.681us         3  6.5600us  1.0700us  17.235us  cuDeviceGet
          0.00%  11.049us         1  11.049us  11.049us  11.049us  cuMemcpyDtoHAsync
          0.00%  5.9280us         3  1.9760us     486ns  3.7500us  cuDeviceGetCount
          0.00%  3.9700us         4     992ns     821ns  1.4170us  cuDeviceGetAttribute
          0.00%  2.9060us         2  1.4530us     469ns  2.4370us  cuEventCreate
          0.00%  2.1130us         1  2.1130us  2.1130us  2.1130us  cuEventRecord
          0.00%  1.9380us         3     646ns     257ns  1.2730us  cuCtxSetCurrent
          0.00%  1.2690us         1  1.2690us  1.2690us  1.2690us  cuModuleGetFunction
          0.00%  1.0160us         1  1.0160us  1.0160us  1.0160us  cuEventSynchronize
          0.00%     941ns         1     941ns     941ns     941ns  cuDeviceComputeCapability
          0.00%     903ns         1     903ns     903ns     903ns  cuCtxGetCurrent
          0.00%     429ns         1     429ns     429ns     429ns  cuMemFree

          ==8811== OpenACC (excl):
          Time(%)      Time     Calls       Avg       Min       Max  Name
          50.16%  136.42ms         5  27.284ms  223.56us  35.061ms  acc_wait@byteswap.h:344
          39.15%  106.47ms         1  106.47ms  106.47ms  106.47ms  acc_device_init
          7.84%  21.312ms      2049  10.401us  4.8300us  169.38us  acc_wait
          2.83%  7.7034ms      2049  3.7590us  3.5020us  29.916us  acc_enqueue_upload
          0.01%  35.966us         4  8.9910us  7.1280us  12.875us  acc_enqueue_launch@byteswap.h:344 (main_344_gpu)
          0.01%  20.712us         4  5.1780us  2.8470us  10.060us  acc_compute_construct@byteswap.h:339
          0.01%  20.466us         1  20.466us  20.466us  20.466us  acc_enqueue_download@byteswap.h:344
          0.00%       0ns         3       0ns       0ns       0ns  acc_delete@(OpenACC API):1
          0.00%       0ns         3       0ns       0ns       0ns  acc_alloc@(OpenACC API):1
        }
      \end{alltt}
    \end{paragraph}
    \begin{paragraph}{}
      Under this $ixykj$ regimen, the \texttt{\#pragma acc loop reduction} directive around the $k$ loop was replaced with a
      \texttt{\#pragma acc loop independent} directive since perfunctory tests with it revealed a decrease in performance
      when attempting an $ixyjk$ reduction model of parallelism.
    \end{paragraph}
  \end{subsection}
  \begin{subsection}{Scale-Out}
    \begin{paragraph}{}
      Development of the scale-out implementation borrowed the core of the OpenACC implementation from the scale-up implementation and was ported
      to use the MPI library functions. This port employed the RMA communication mechanism with Post-Start-Complete-Wait synchronization since
      the cyclic rotation between MPI processes must be marshalled in such a way that the present data is not overwritten.
    \end{paragraph}
    \begin{paragraph}{Array Distribution}
      During program startup, the root process $P_{0,0}$ fills the arrays $A$, $B$, and $C$ and issues a \texttt{MPI\_Win\_post} call to \texttt{MPI\_COMM\_WORLD}
      to notify the other processes and itself that the arrays are ready for retrieval. Each process $P_{x,y}$ issues a \texttt{MPI\_Get} request
      to the root process $P_{0,0}$ for the properly sheared 2D subarrays $A_{x,(y+x)\ mod\ c}$ and $B_{(x+y)\ mod\ b, y}$ along with the stationary
      2D subarray $C_{x,y}$. These are stored in individual process memory and opened via another RMA window so that the requisite neighbor processes
      can access them during the cyclic rotation phases.
    \end{paragraph}
    \begin{paragraph}{Cyclic Rotation}
      Within each process, the memory allocated for the $A$ and $B$ subarrays is actually twice that required. This is to enable whole-block copying
      between neighbor processes. The additional space serves as a send buffer during cyclic rotation and prevents corruption. Each process $P_{x,y}$
      copies the entirety of the $A$ ($B$) submatrix and then issues a \texttt{MPI\_Win\_post} call for the corresponding window to a group comprised of
      only its east (south) neighbor so that it can receive the other's copy of the corresponding submatrix. Immediately after the \texttt{MPI\_Post}
      calls are sent, the process $P_{x,y}$ then issues a \texttt{MPI\_Win\_Start} call to its west (north) neighbor so that it can send its prior submatrix
      $A$ ($B$) from the ghost block. Once that neighbor has posted its readiness, the process $P_{x,y}$ issues a \texttt{MPI\_Put} to deliver the data.
      The transaction is finalized with a call to \texttt{MPI\_Win\_complete} on the corresponding window, and then the process issues a \texttt{MPI\_Win\_wait}
      to block until it has assurance that its supplier has finished the incoming transaction before proceeding to the multiplication phase.
    \end{paragraph}
    \begin{paragraph}{Local Multiplication}
      The local multiplication is implemented as a simple \emph{ikj} block multiplication. Some attempts have been made to optimize it
      with OpenMP, but the parallelizations have turned out to be worse than the serial in local development tests.
      After each local multiplication is finished, the process $P\_{x,y}$ loops until it has processed the block row $x$ and block column $y$
      at most once.
    \end{paragraph}
    \begin{paragraph}{Collection}
      When all the work has finished, the various processes report back to the root process via individual \texttt{MPI\_Put} calls to load balance
      the communication. It remains to be seen whether a more complex call like \texttt{MPI\_Gather} can be used to optimize the implementation.
    \end{paragraph}
  \end{subsection}
\end{section}
\printbibliography

\end{document}
